-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
current project tree
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

lane-detection
│   .gitignore
│   LICENSE
│   README.md
│
├── data
│   ├── frames_sample/
│   │       frame_t1_f60.png
│   │       frame_t4_f240.png
│   │       ...
│   │       frame_t295_f17700.png
│   │
│   ├── processed/
│   │       highway_clip.mp4
│   │
│   └── raw/
│           .gitkeep
│
├── notebooks/
│       01_explore_frames.ipynb
│       02_roi_exploration.ipynb
│       03_color_thresholding.ipynb
│       04_canny_edge_detection.ipynb
│
├── output/
│   ├── final_videos/
│   │       .gitkeep
│   │
│   └── logs/
│       │   algorithm_log.txt
│       │
│       └── debug_frames/
│               .gitkeep
│
├── report/
│       .gitkeep
│
└── src/
    │   lane_detection.py
    │   pipeline.py
    │   pre_processing.py
    │
    ├── enhancements/
    │       .gitkeep
    │
    └── utils/
            debug.py
            drawing.py
            geometry.py
            image_ops.py
            video.py
            __init__.py

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
stage 1 - Pre-processing Stage – Frames Extraction
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Pre-processing Stage – Frames Extraction

Status: Completed successfully
Description:
Extracted representative frames from the main video (highway_clip.mp4) for exploratory analysis and algorithm development.
Two extraction strategies were used:

Uniform sampling — saved every 50 frames into data/frames/ to obtain general road scenarios.

Targeted extraction — saved 36 manually selected timestamps into data/frames_sample/ representing:

straight-lane conditions

strong shadows

bright lighting

lane boundaries variability

lane change events

traffic interaction scenarios

These frames will be used throughout development to tune ROI, Canny thresholds, line detection, and lane-change logic.

Next steps:
Start exploring the selected frames in a dedicated notebook (01_explore_frames.ipynb) and identify stable patterns in lane geometry, colors, and noise sources.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Stage 2 — Frame Exploration & Pattern Identification
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Stage 2 — Frame Exploration & Pattern Identification

Status: Completed successfully
Description:
Loaded and visualized the 36 curated sample frames inside the notebook (01_explore_frames.ipynb).
The goal of this stage is to understand the visual variability in the dataset and identify parameters that must be robust across all scenarios.

The frame exploration focused on:

Differences in lighting 

Road curvature and perspective distortion

Variability in lane-marker width, color (white/yellow), and visibility

Occlusions from nearby vehicles

Frames where the host vehicle performs a lane change

Frames where other vehicles merge or cross lanes

These observations will guide the selection of:

ROI (Region of Interest) shape

Color threshold ranges for lane markings

Canny thresholds

Hough parameter ranges

Smoothing/temporal filtering strategy

Lane-change detection logic

Key accomplishments:

Created notebook 01_explore_frames.ipynb with organized frame categorization

Defined 4 frame categories for targeted testing:
ROI_FRAME_NAMES (8 frames) — for ROI mask tuning
CANNY_FRAME_NAMES (8 frames) — for edge detection parameter tuning
HOUGH_FRAME_NAMES (8 frames) — for line detection validation
LANE_CHANGE_FRAME_NAMES (10 frames) — for lane-change event detection

Identified visual challenges:
Shadow interference on white lane markings
Bright reflections and glare
Curved road sections with perspective distortion
Adjacent vehicles partially occluding lane boundaries
Yellow and white lane transitions

These categorized frames serve as test sets for each pipeline stage, ensuring algorithm robustness.

Next steps:
Define ROI geometry and test it across representative frames (02_roi_exploration.ipynb).

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Stage 2.5 — ROI (Region of Interest) Exploration
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Stage 2.5 — ROI (Region of Interest) Exploration

Status: Completed successfully
Description:
Defined and tuned a trapezoidal Region of Interest (ROI) mask to focus the lane detection algorithm on the relevant road area.
The goal of this stage was to exclude irrelevant image regions (sky, roadside, distant lanes) and concentrate computational resources on the immediate driving lane.

The ROI exploration focused on:

Testing different trapezoid shapes across various road perspectives

Finding optimal vertical cutoff (how much road ahead to include)

Balancing between capturing curved lanes and excluding irrelevant regions

Validating ROI stability across straight roads, curves, and lane changes

The trapezoid shape was chosen because it mimics natural perspective: lanes appear wide near the camera and converge toward the horizon.
This geometric constraint matches road physics and reduces false positives from non-road features.

The finalized ROI parameters successfully isolate the driving lane across all test scenarios, creating a focused region for subsequent color and edge detection stages.

Next steps:
Apply color thresholding within the ROI to isolate white and yellow lane markings.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Stage 3 — Color Thresholding
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Stage 3 — Color Thresholding

Status: Completed successfully
Description:
Implemented HSV-based color segmentation to isolate white and yellow lane markings from the road scene.
This stage focused on finding robust threshold values that work across varying lighting conditions (shadows, bright sun, tunnels).

The color thresholding exploration focused on:

Converting ROI-masked frames from BGR to HSV color space for more intuitive color-based segmentation

Identifying HSV ranges that reliably capture white lane markings (high brightness, low saturation)

Identifying HSV ranges that reliably capture yellow lane markings (hue around 20-30 degrees)

Testing threshold stability across shadows, bright reflections, and varying road textures

Combining white and yellow detections to create unified lane masks

Applying morphological operations to remove small noise and fill gaps in detected lanes

The HSV color space was chosen because it separates brightness (Value) from color information (Hue, Saturation),
making it more robust to lighting variations than RGB/BGR. White lanes are characterized by high brightness and low color saturation,
while yellow lanes have a distinct hue around 20-30 degrees.

The finalized thresholds successfully produce clean binary masks where lane markings appear as white (255) and background as black (0).
These binary masks eliminate most road texture, shadows, and non-lane features, creating a focused input for edge detection.

Validation was performed across 16 diverse test frames representing different lighting conditions, road curvatures, and traffic scenarios.

Next steps:
Apply Canny edge detection to the color-thresholded masks to extract precise edge contours for line detection.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Stage 4 — Canny Edge Detection
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Stage 4 — Canny Edge Detection

Status: Completed successfully
Description:
Applied Canny edge detection to the color-thresholded lane masks to extract precise edge boundaries.
This stage focused on finding optimal Canny threshold parameters that produce clean, continuous edges while suppressing noise.

The Canny edge detection exploration focused on:

Applying Gaussian blur preprocessing to reduce high-frequency noise before edge detection

Testing multiple threshold pairs to find the optimal balance:
Low thresholds (30/90) — too sensitive, captures noise and texture
Medium thresholds (50/150) — optimal, captures strong and moderate edges
High thresholds (70/210, 100/200) — too conservative, misses valid lane edges

Validating edge continuity across straight lanes, curves, and lane-change scenarios

Ensuring edge detection works consistently across varying lighting conditions (shadows, reflections, tunnels)

Canny edge detection uses hysteresis thresholding with two threshold values:
High threshold identifies strong edges (definite lane boundaries)
Low threshold extends edges from strong points, capturing weaker but connected edge segments

The Gaussian blur step is critical because Canny is sensitive to noise. By smoothing the image first,
we suppress texture noise while preserving true lane edge structures.

The finalized parameters (low=50, high=150, blur_kernel=5) successfully produce binary edge maps with well-defined, continuous lane boundaries.
These edges eliminate the interior regions of lane markings and focus only on the boundary contours,
which is essential for accurate line fitting in the next stage.

Validation was performed across all 16 test frames, confirming robust edge extraction in diverse scenarios.

Next steps:
Apply Hough line detection to the edge maps to identify candidate line segments, then fit polynomial curves to represent lane boundaries.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Stage 5 — Hough Line Detection & Lane Boundary Fitting
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Stage 5 — Hough Line Detection & Lane Boundary Fitting

Status: Completed successfully
Description:
Implemented Probabilistic Hough Transform to detect line segments from edge maps, followed by slope-based filtering and linear regression fitting to extract accurate lane boundaries.
This stage focused on converting edge pixels into meaningful line representations and merging multiple line segments into single, stable lane boundaries.

The line detection and fitting exploration focused on:

Applying Probabilistic Hough Transform to edge maps to extract candidate line segments

Testing multiple Hough parameter combinations:
threshold (15-50) — minimum votes required to detect a line
minLineLength (30-60) — minimum segment length in pixels
maxLineGap (20-150) — maximum gap allowed between connected segments

Filtering detected lines by slope to separate left lane (negative slope) from right lane (positive slope)

Removing near-horizontal lines (|slope| < 0.5) which represent noise or road markings

Removing near-vertical lines (|slope| > 2.0) which are too steep to be valid lane boundaries

Applying linear regression (np.polyfit) to merge multiple line segments into single, optimal lane lines

Using x = f(y) instead of y = f(x) to handle near-vertical lane lines without numerical instability

Extrapolating fitted lines to span the full ROI height for complete lane visualization

Generating lane polygon overlays to visualize the detected driving lane

Probabilistic Hough Transform was chosen over standard Hough Transform because it:
- Returns line segment endpoints directly (not just parameters)
- Is computationally more efficient for large images
- Provides better control over segment length and gap tolerance

The slope-based filtering step is critical because it:
- Separates left and right lane boundaries based on slope sign
- Eliminates horizontal road markings (crosswalks, arrows) that could be misinterpreted as lanes
- Rejects overly steep lines that are likely noise or non-lane features

Linear regression fitting (fit_lane_line) is the key innovation that produces stable, accurate lane lines:
- Collects all endpoint coordinates from multiple small line segments
- Fits a single best-fit straight line through all points using least-squares regression
- Produces one continuous, smooth line per lane instead of many fragmented segments
- Significantly reduces jitter and improves visual quality compared to selecting individual segments

The x = f(y) formulation is mathematically essential because lane lines are nearly vertical.
If we used y = f(x), vertical lines would have infinite slope, causing numerical errors.
By fitting x as a function of y, we maintain numerical stability across all lane angles.

The finalized pipeline successfully produces:
- Straight, continuous lane lines fitted to detected edge segments
- Stable left/right lane separation with minimal false positives
- Full-height lane boundaries spanning from ROI bottom to top
- Semi-transparent lane polygon visualization showing the detected driving corridor

Validation was performed across all 36 test frames, demonstrating robust lane detection in diverse scenarios:
- Straight roads with clear lane markings
- Curved roads requiring perspective-aware fitting
- Shadow conditions with partial lane occlusion
- Bright lighting with reflections and glare
- Adjacent traffic with potential interference

Multi-frame testing results:
- Both lanes detected successfully in majority of frames
- Linear regression significantly improved line straightness and stability
- Fitted lines remain consistent across consecutive frames, reducing jitter

Next steps:
Implement temporal smoothing across video frames to further stabilize lane detection and reduce frame-to-frame jitter.
Add lane-change detection logic to identify when the vehicle is transitioning between lanes.
Integrate the complete pipeline into a video processing script for real-time lane tracking.
